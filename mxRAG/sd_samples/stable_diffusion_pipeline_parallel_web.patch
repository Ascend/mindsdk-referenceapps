diff --git a/MindIE/MindIE-Torch/built-in/foundation/stable_diffusion/stable_diffusion_pipeline_parallel.py b/MindIE/MindIE-Torch/built-in/foundation/stable_diffusion/stable_diffusion_pipeline_parallel.py
index 76c7e606c..3baa44a78 100644
--- a/MindIE/MindIE-Torch/built-in/foundation/stable_diffusion/stable_diffusion_pipeline_parallel.py
+++ b/MindIE/MindIE-Torch/built-in/foundation/stable_diffusion/stable_diffusion_pipeline_parallel.py
@@ -13,13 +13,20 @@
 # limitations under the License.
 
 import argparse
+import base64
 import csv
 import json
 import os
+import io
 import time
 from typing import Callable, List, Optional, Union
 import numpy as np
 
+import uvicorn
+from fastapi import FastAPI, HTTPException
+from pydantic import BaseModel
+from fastapi.responses import Response
+
 import torch
 import mindietorch
 from mindietorch import _enums
@@ -29,6 +36,10 @@ from diffusers import DPMSolverMultistepScheduler, EulerDiscreteScheduler, DDIMS
 from background_runtime import BackgroundRuntime, RuntimeIOInfo
 from background_runtime_cache import BackgroundRuntimeCache, RuntimeIOInfoCache
 
+app = FastAPI()
+pipe = None
+args = None
+
 clip_time = 0
 unet_time = 0
 vae_time = 0
@@ -37,84 +48,6 @@ p2_time = 0
 p3_time = 0
 scheduler_time = 0
 
-class PromptLoader:
-    def __init__(
-            self,
-            prompt_file: str,
-            prompt_file_type: str,
-            batch_size: int,
-            num_images_per_prompt: int = 1,
-    ):
-        self.prompts = []
-        self.catagories = ['Not_specified']
-        self.batch_size = batch_size
-        self.num_images_per_prompt = num_images_per_prompt
-
-        if prompt_file_type == 'plain':
-            self.load_prompts_plain(prompt_file)
-
-        elif prompt_file_type == 'parti':
-            self.load_prompts_parti(prompt_file)
-
-        self.current_id = 0
-        self.inner_id = 0
-
-    def __len__(self):
-        return len(self.prompts) * self.num_images_per_prompt
-
-    def __iter__(self):
-        return self
-
-    def __next__(self):
-        if self.current_id == len(self.prompts):
-            raise StopIteration
-
-        ret = {
-            'prompts': [],
-            'catagories': [],
-            'save_names': [],
-            'n_prompts': self.batch_size,
-        }
-        for _ in range(self.batch_size):
-            if self.current_id == len(self.prompts):
-                ret['prompts'].append('')
-                ret['save_names'].append('')
-                ret['catagories'].append('')
-                ret['n_prompts'] -= 1
-
-            else:
-                prompt, catagory_id = self.prompts[self.current_id]
-                ret['prompts'].append(prompt)
-                ret['catagories'].append(self.catagories[catagory_id])
-                ret['save_names'].append(f'{self.current_id}_{self.inner_id}')
-
-                self.inner_id += 1
-                if self.inner_id == self.num_images_per_prompt:
-                    self.inner_id = 0
-                    self.current_id += 1
-
-        return ret
-
-    def load_prompts_plain(self, file_path: str):
-        with os.fdopen(os.open(file_path, os.O_RDONLY), "r") as f:
-            for i, line in enumerate(f):
-                prompt = line.strip()
-                self.prompts.append((prompt, 0))
-
-    def load_prompts_parti(self, file_path: str):
-        with os.fdopen(os.open(file_path, os.O_RDONLY), "r", encoding='utf8') as f:
-            # Skip the first line
-            next(f)
-            tsv_file = csv.reader(f, delimiter="\t")
-            for i, line in enumerate(tsv_file):
-                prompt = line[0]
-                catagory = line[1]
-                if catagory not in self.catagories:
-                    self.catagories.append(catagory)
-
-                catagory_id = self.catagories.index(catagory)
-                self.prompts.append((prompt, catagory_id))
-
 
 class AIEStableDiffusionPipeline(StableDiffusionPipeline):
     device_0 = None
@@ -983,10 +916,19 @@ def parse_arguments():
         help="Steps to use cache data."
     )
 
+    parser.add_argument(
+        "--port",
+        type=int,
+        default=7860,
+        help="The port number used by fastapi."
+    )
+    
     return parser.parse_args()
 
 
 def main():
+    global args
+    global pipe
     args = parse_arguments()
     save_dir = args.save_dir
 
@@ -1006,89 +948,66 @@ def main():
     pipe.compile_aie_model()
     mindietorch.set_device(pipe.device_0)
 
-    skip_steps = [0] * args.steps
+    args.skip_steps = [0] * args.steps
 
-    flag_cache = 0
+    args.flag_cache = 0
     if args.use_cache:
-        flag_cache = 1
+        args.flag_cache = 1
         for i in args.cache_steps.split(','):
             if int(i) >= args.steps:
                 continue
-            skip_steps[int(i)] = 1
-
-    use_time = 0
-    prompt_loader = PromptLoader(args.prompt_file,
-                                 args.prompt_file_type,
-                                 args.batch_size,
-                                 args.num_images_per_prompt)
-
-    infer_num = 0
-    image_info = []
-    current_prompt = None
-    for i, input_info in enumerate(prompt_loader):
-        prompts = input_info['prompts']
-        catagories = input_info['catagories']
-        save_names = input_info['save_names']
-        n_prompts = input_info['n_prompts']
-
-        print(f"[{infer_num + n_prompts}/{len(prompt_loader)}]: {prompts}")
-        infer_num += args.batch_size
-
-        start_time = time.time()
-        if args.scheduler == "DDIM":
-            images = pipe.ascendie_infer_ddim(
-                prompts,
-                num_inference_steps=args.steps,
-                skip_steps=skip_steps,
-                flag_cache=flag_cache,
-            )
-        else:
-            images = pipe.ascendie_infer(
-                prompts,
-                num_inference_steps=args.steps,
-                skip_steps=skip_steps,
-                flag_cache=flag_cache,
-            )
+            args.skip_steps[int(i)] = 1
 
-        if i > 4: # do not count the time spent inferring the first 0 to 4 images
-            use_time += time.time() - start_time
 
-        for j in range(n_prompts):
-            image_save_path = os.path.join(save_dir, f"{save_names[j]}.png")
-            image = images[0][j]
-            image.save(image_save_path)
+class ImageRequest(BaseModel):
+    prompt: str
+    output_format: str
+	size: str = "512*512"
 
-            if current_prompt != prompts[j]:
-                current_prompt = prompts[j]
-                image_info.append({'images': [], 'prompt': current_prompt, 'category': catagories[j]})
 
-            image_info[-1]['images'].append(image_save_path)
+@app.post("/text2img")
+async def text2image(image_request: ImageRequest):
+    prompt = image_request.prompt
+    output_format = image_request.output_format
+	height = int(image_request.size.split("*")[0])
+	width = int(image_request.size.split("*")[1])
+    if output_format.lower() not in ["png", "jpeg", "jpg", "webp"]:
+        raise HTTPException(status_code=400, detail="Invalid output format")
 
-    infer_num = infer_num - 5 # do not count the time spent inferring the first 5 images
-    print(f"[info] infer number: {infer_num}; use time: {use_time:.3f}s\n"
-          f"average time: {use_time / infer_num:.3f}s\n"
-          f"clip time: {clip_time / infer_num:.3f}s\n"
-          f"unet time: {unet_time / infer_num:.3f}s\n"
-          f"vae time: {vae_time / infer_num:.3f}s\n"
-          f"p1 time: {p1_time / infer_num:.3f}s\n"
-          f"p2 time: {p2_time / infer_num:.3f}s\n"
-          )
-    if hasattr(pipe, 'device_1'):
-        if (pipe.unet_bg):
-            pipe.unet_bg.stop()
+    if output_format == "jpg":
+        output_format = "jpeg"
 
-        if (pipe.unet_bg_cache):
-            pipe.unet_bg_cache.stop()
+    global args
+    global pipe
 
-    # Save image information to a json file
-    if os.path.exists(args.info_file_save_path):
-        os.remove(args.info_file_save_path)
+    if args.scheduler == "DDIM":
+        images = pipe.ascendie_infer_ddim(
+            [prompt],
+			height = height,
+			width = width,
+            num_inference_steps=args.steps,
+            skip_steps=args.skip_steps,
+            flag_cache=args.flag_cache,
+        )
+    else:
+        images = pipe.ascendie_infer(
+            [prompt],
+			height = height,
+			width = width,
+            num_inference_steps=args.steps,
+            skip_steps=args.skip_steps,
+            flag_cache=args.flag_cache,
+        )
 
-    with os.fdopen(os.open(args.info_file_save_path, os.O_RDWR | os.O_CREAT, 0o640), "w") as f:
-        json.dump(image_info, f)
+    image = images[0][0]
 
-    mindietorch.finalize()
+    image_byte_arr = io.BytesIO()
+    image.save(image_byte_arr, format=output_format)
+    image_byte_arr.seek(0)    
+	return base64.b64encode(image_byte_arr.read())
+    # return Response(content=image_byte_arr.getvalue(), media_type=f"image/{output_format.lower()}")
 
 
 if __name__ == "__main__":
     main()
+    uvicorn.run(app, host="0.0.0.0", port=args.port)
\ No newline at end of file
