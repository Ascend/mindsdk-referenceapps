diff --git a/whisper/decoding.py b/whisper/decoding.py
index 49485d0..495fe45 100644
--- a/whisper/decoding.py
+++ b/whisper/decoding.py
@@ -2,6 +2,7 @@ from dataclasses import dataclass, field, replace
 from typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Sequence, Tuple, Union
 
 import numpy as np
+import os
 import torch
 import torch.nn.functional as F
 from torch import Tensor
@@ -49,12 +50,24 @@ def detect_language(
 
     # skip encoder forward pass if already-encoded audio features were given
     if mel.shape[-2:] != (model.dims.n_audio_ctx, model.dims.n_audio_state):
+        encoder_ts_model = torch.jit.trace(model.encoder, mel)
+        encoder_ts_model.save(
+            "/tmp/models/encoder.ts")
+        torch.onnx.export(
+            model.encoder,
+            (mel),
+            "/tmp/models/onnx/encode/encoder.onnx",
+            opset_version=11,
+            input_names=["mel"],
+            output_names=["ret"]
+        )
+
         mel = model.encoder(mel)
 
     # forward pass using a single token, startoftranscript
     n_audio = mel.shape[0]
     x = torch.tensor([[tokenizer.sot]] * n_audio).to(mel.device)  # [n_audio, 1]
-    logits = model.logits(x, mel)[:, 0]
+    logits = model.logits(x, mel)[0][:, 0]
 
     # collect detected languages; suppress all non-language tokens
     mask = torch.ones(logits.shape[-1], dtype=torch.bool)
@@ -145,36 +158,74 @@ class PyTorchInference(Inference):
     def __init__(self, model: "Whisper", initial_token_length: int):
         self.model: "Whisper" = model
         self.initial_token_length = initial_token_length
-        self.kv_cache = {}
-        self.hooks = []
-
-        key_modules = [block.attn.key for block in self.model.decoder.blocks]
-        value_modules = [block.attn.value for block in self.model.decoder.blocks]
-        self.kv_modules = key_modules + value_modules
+        self.cache_dyn = None
+        self.cache_sta = None
 
     def logits(self, tokens: Tensor, audio_features: Tensor) -> Tensor:
-        if not self.kv_cache:
-            self.kv_cache, self.hooks = self.model.install_kv_cache_hooks()
-
         if tokens.shape[-1] > self.initial_token_length:
             # only need to use the last token except in the first forward pass
             tokens = tokens[:, -1:]
+            pos_embed = self.model.decoder.positional_embedding[self.cache_dyn.shape[3]]
+            torch.onnx.export(
+                self.model.decoder,
+                (tokens, audio_features, pos_embed, self.cache_dyn, self.cache_sta),
+                "/tmp/models/onnx/decode/decoder_decode.onnx",
+                opset_version=11,
+                input_names=["tokens", "audio_features", "pos_embed", "cache_dyn", "cache_sta"],
+                output_names=["logits", "new_cache_dyn", "new_cache_sta"],
+                dynamic_axes={
+                    "cache_dyn": {3: "ntokens"},
+                    "new_cache_dyn": {3: "ntokens"}
+                }                  
+            )
+            decoder_decode_ts_model = torch.jit.trace(
+                self.model.decoder,
+                (tokens, audio_features, pos_embed, self.cache_dyn, self.cache_sta)
+            )
+            decoder_decode_ts_model.save(
+                "/tmp/models/decoder_decode.ts")
+            logits, cache_dyn, _ = self.model.decoder(
+                tokens, audio_features, pos_embed, self.cache_dyn, self.cache_sta)
+            os.sys.exit(0)
+            self.cache_dyn = cache_dyn
+        else:
+            pos_embed = self.model.decoder.positional_embedding[:tokens.shape[-1]]
+            torch.onnx.export(
+                self.model.decoder,
+                (tokens, audio_features, pos_embed),
+                "/tmp/models/onnx/prefill/decoder_prefill.onnx",
+                opset_version=11,
+                input_names=["tokens", "audio_features", "pos_embed"],
+                output_names=["logits", "cache_dyn", "cache_sta"],
+                dynamic_axes={
+                    "tokens": {1: "ntokens"},
+                    "pos_embed": {0: "ntokens"},
+                    "logits": {1: "ntokens"},
+                    "cache_dyn": {3: "ntokens"}
+                }
+            )
+            decoder_prefill_ts_model = torch.jit.trace(
+                self.model.decoder,
+                (tokens, audio_features, pos_embed)
+            )
+            decoder_prefill_ts_model.save(
+                "/tmp/models/decoder_prefill.ts")
+            logits, cache_dyn, cache_sta = self.model.decoder(tokens, audio_features, pos_embed)
+            self.cache_dyn = cache_dyn
+            self.cache_sta = cache_sta
 
-        return self.model.decoder(tokens, audio_features, kv_cache=self.kv_cache)
+        return logits
 
     def cleanup_caching(self):
-        for hook in self.hooks:
-            hook.remove()
-
-        self.kv_cache = {}
-        self.hooks = []
+        self.cache_dyn = None
+        self.cache_sta = None
 
     def rearrange_kv_cache(self, source_indices):
         if source_indices != list(range(len(source_indices))):
-            for module in self.kv_modules:
-                # update the key/value cache to contain the selected sequences
-                self.kv_cache[module] = self.kv_cache[module][source_indices].detach()
-
+            blocks = self.cache_dyn.shape[0]
+            for i in range(blocks):
+                for j in range(2): # k and v 2 items
+                    self.cache_dyn[i][j] = self.cache_dyn[i][j][source_indices]
 
 class SequenceRanker:
     def rank(
diff --git a/whisper/model.py b/whisper/model.py
index a678283..2a95e28 100644
--- a/whisper/model.py
+++ b/whisper/model.py
@@ -1,6 +1,7 @@
 import base64
 import gzip
 from dataclasses import dataclass
+import os
 from typing import Dict, Iterable, Optional
 
 import numpy as np
@@ -68,6 +69,63 @@ class MultiHeadAttention(nn.Module):
         self.value = Linear(n_state, n_state)
         self.out = Linear(n_state, n_state)
 
+    def encoder_forward(self, x: Tensor):
+        q = self.query(x)
+        k = self.key(x)
+        v = self.value(x)
+        wv, qk = self.qkv_attention(q, k, v)
+        return self.out(wv)
+
+    def prefill_self_attn_forward(
+        self,
+        x: Tensor,
+        mask: Tensor,
+    ):
+        q = self.query(x)
+        k = self.key(x)
+        v = self.value(x)
+        cache_dyn = torch.stack([k, v])
+        wv, _ = self.qkv_attention(q, k, v, mask)
+        return self.out(wv), cache_dyn
+    
+    def prefill_cross_attn_forward(
+        self,
+        x: Tensor,
+        xa: Tensor,
+    ):
+        q = self.query(x)
+        k = self.key(xa)
+        v = self.value(xa)
+        cache_sta = torch.stack([k, v])
+        wv, _ = self.qkv_attention(q, k, v)
+        return self.out(wv), cache_sta
+    
+    def decode_self_attn_forward(
+        self,
+        x: Tensor,
+        mask: Tensor,
+        cache_dyn: Tensor
+    ):
+        q = self.query(x)
+        token_k = self.key(x)
+        k = torch.cat([cache_dyn[0], token_k], dim=1).detach()
+        token_v = self.value(x)
+        v = torch.cat([cache_dyn[1], token_v], dim=1).detach()
+        new_cache_dyn = torch.stack([k, v])
+        wv, _ = self.qkv_attention(q, k, v, mask)
+        return self.out(wv), new_cache_dyn
+
+    def decode_cross_attn_forward(
+        self,
+        x: Tensor,
+        cache_sta: Tensor
+    ):
+        q = self.query(x)
+        k = cache_sta[0]
+        v = cache_sta[1]
+        wv, _ = self.qkv_attention(q, k, v)
+        return self.out(wv)
+
     def forward(
         self,
         x: Tensor,
@@ -126,6 +184,39 @@ class ResidualAttentionBlock(nn.Module):
         )
         self.mlp_ln = LayerNorm(n_state)
 
+    def encoder_forward(self, x: Tensor):
+        x = x + self.attn.encoder_forward(self.attn_ln(x))
+        x = x + self.mlp(self.mlp_ln(x))
+        return x
+
+    def prefill_forward(
+        self,
+        x: Tensor,
+        xa: Tensor,
+        mask: Tensor,
+    ):
+        self_attn_out, new_cache_dyn = self.attn.prefill_self_attn_forward(self.attn_ln(x), mask)
+        x = x + self_attn_out
+        cross_attn_out, new_cache_sta = self.cross_attn.prefill_cross_attn_forward(self.cross_attn_ln(x), xa)
+        x = x + cross_attn_out
+        x = x + self.mlp(self.mlp_ln(x))
+        return x, new_cache_dyn, new_cache_sta
+
+    def decode_forward(
+        self,
+        x: Tensor,
+        xa: Tensor,
+        mask: Tensor,
+        cache_dyn: Tensor,
+        cache_sta: Tensor
+    ):
+        self_attn_out, new_cache_dyn = self.attn.decode_self_attn_forward(self.attn_ln(x), mask, cache_dyn)
+        x = x + self_attn_out
+        cross_attn_out = self.cross_attn.decode_cross_attn_forward(self.cross_attn_ln(x), cache_sta)
+        x = x + cross_attn_out
+        x = x + self.mlp(self.mlp_ln(x))
+        return x, new_cache_dyn
+
     def forward(
         self,
         x: Tensor,
@@ -163,11 +254,10 @@ class AudioEncoder(nn.Module):
         x = F.gelu(self.conv2(x))
         x = x.permute(0, 2, 1)
 
-        assert x.shape[1:] == self.positional_embedding.shape, "incorrect audio shape"
         x = (x + self.positional_embedding).to(x.dtype)
 
         for block in self.blocks:
-            x = block(x)
+            x = block.encoder_forward(x)
 
         x = self.ln_post(x)
         return x
@@ -193,29 +283,56 @@ class TextDecoder(nn.Module):
         mask = torch.empty(n_ctx, n_ctx).fill_(-np.inf).triu_(1)
         self.register_buffer("mask", mask, persistent=False)
 
-    def forward(self, x: Tensor, xa: Tensor, kv_cache: Optional[dict] = None):
-        """
-        x : torch.LongTensor, shape = (batch_size, <= n_ctx)
-            the text tokens
-        xa : torch.Tensor, shape = (batch_size, n_audio_ctx, n_audio_state)
-            the encoded audio features to be attended on
-        """
-        offset = next(iter(kv_cache.values())).shape[1] if kv_cache else 0
-        x = (
-            self.token_embedding(x)
-            + self.positional_embedding[offset : offset + x.shape[-1]]
-        )
-        x = x.to(xa.dtype)
+    def prefill(self, x: Tensor, xa: Tensor, pos_embed: Tensor):
+        x = (self.token_embedding(x) + pos_embed).to(xa.dtype)
 
+        cache_dyn_list = []
+        cache_sta_list = []
         for block in self.blocks:
-            x = block(x, xa, mask=self.mask, kv_cache=kv_cache)
+            x, new_cache_dyn, new_cache_sta = block.prefill_forward(x, xa, self.mask)
+            cache_dyn_list.append(new_cache_dyn)
+            cache_sta_list.append(new_cache_sta)
+
+        cache_dyn = torch.stack(cache_dyn_list)
+        cache_sta = torch.stack(cache_sta_list)
 
         x = self.ln(x)
         logits = (
             x @ torch.transpose(self.token_embedding.weight.to(x.dtype), 0, 1)
         ).float()
 
-        return logits
+        return logits, cache_dyn, cache_sta
+    
+    def decode(self, x: Tensor, xa: Tensor, pos_embed: Tensor, cache_dyn: Tensor, cache_sta: Tensor):
+        x = (self.token_embedding(x) + pos_embed).to(xa.dtype)
+
+        cache_dyn_list = []
+        for idx, block in enumerate(self.blocks):
+            x, new_cache_dyn = block.decode_forward(x, xa, self.mask, cache_dyn[idx], cache_sta[idx])
+            cache_dyn_list.append(new_cache_dyn)
+
+        new_cache_dyn = torch.stack(cache_dyn_list)
+
+        x = self.ln(x)
+        logits = (
+            x @ torch.transpose(self.token_embedding.weight.to(x.dtype), 0, 1)
+        ).float()
+
+        return logits, new_cache_dyn
+
+    def forward(
+        self,
+        x: Tensor,
+        xa: Tensor,
+        pos_embed: Tensor = None,
+        cache_dyn: Tensor = None,
+        cache_sta: Tensor = None,
+    ):
+        if cache_dyn is None:
+            logits, cache_dyn, cache_sta = self.prefill(x, xa, pos_embed)
+        else:
+            logits, cache_dyn = self.decode(x, xa, pos_embed, cache_dyn, cache_sta)
+        return logits, cache_dyn, cache_sta
 
 
 class Whisper(nn.Module):
@@ -257,7 +374,8 @@ class Whisper(nn.Module):
         return self.encoder(mel)
 
     def logits(self, tokens: torch.Tensor, audio_features: torch.Tensor):
-        return self.decoder(tokens, audio_features)
+        pos_embed = self.decoder.positional_embedding[:tokens.shape[-1]]
+        return self.decoder(tokens, audio_features, pos_embed)
 
     def forward(
         self, mel: torch.Tensor, tokens: torch.Tensor
