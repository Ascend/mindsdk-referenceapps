diff --git a/whisper/decoding.py b/whisper/decoding.py
index 49485d0..4dccc86 100644
--- a/whisper/decoding.py
+++ b/whisper/decoding.py
@@ -6,6 +6,7 @@ import torch
 import torch.nn.functional as F
 from torch import Tensor
 from torch.distributions import Categorical
+import mindietorch
 
 from .audio import CHUNK_LENGTH
 from .tokenizer import Tokenizer, get_tokenizer
@@ -14,6 +15,7 @@ from .utils import compression_ratio
 if TYPE_CHECKING:
     from .model import Whisper
 
+mindietorch.set_device(0)
 
 @torch.no_grad()
 def detect_language(
@@ -54,7 +56,7 @@ def detect_language(
     # forward pass using a single token, startoftranscript
     n_audio = mel.shape[0]
     x = torch.tensor([[tokenizer.sot]] * n_audio).to(mel.device)  # [n_audio, 1]
-    logits = model.logits(x, mel)[:, 0]
+    logits = model.logits(x, mel)[0][:, 0]
 
     # collect detected languages; suppress all non-language tokens
     mask = torch.ones(logits.shape[-1], dtype=torch.bool)
@@ -145,36 +147,35 @@ class PyTorchInference(Inference):
     def __init__(self, model: "Whisper", initial_token_length: int):
         self.model: "Whisper" = model
         self.initial_token_length = initial_token_length
-        self.kv_cache = {}
-        self.hooks = []
-
-        key_modules = [block.attn.key for block in self.model.decoder.blocks]
-        value_modules = [block.attn.value for block in self.model.decoder.blocks]
-        self.kv_modules = key_modules + value_modules
+        self.cache_dyn = None
+        self.cache_sta = None
 
     def logits(self, tokens: Tensor, audio_features: Tensor) -> Tensor:
-        if not self.kv_cache:
-            self.kv_cache, self.hooks = self.model.install_kv_cache_hooks()
-
         if tokens.shape[-1] > self.initial_token_length:
             # only need to use the last token except in the first forward pass
             tokens = tokens[:, -1:]
+            pos_embed = self.model.decoder.positional_embedding[self.cache_dyn.shape[3]]
+            logits, cache_dyn, _ = self.model.decoder(
+                tokens, audio_features, pos_embed, self.cache_dyn, self.cache_sta)
+            self.cache_dyn = cache_dyn
+        else:
+            pos_embed = self.model.decoder.positional_embedding[:tokens.shape[-1]]
+            logits, cache_dyn, cache_sta = self.model.decoder(tokens, audio_features, pos_embed)
+            self.cache_dyn = cache_dyn
+            self.cache_sta = cache_sta
 
-        return self.model.decoder(tokens, audio_features, kv_cache=self.kv_cache)
+        return logits
 
     def cleanup_caching(self):
-        for hook in self.hooks:
-            hook.remove()
-
-        self.kv_cache = {}
-        self.hooks = []
+        self.cache_dyn = None
+        self.cache_sta = None
 
     def rearrange_kv_cache(self, source_indices):
         if source_indices != list(range(len(source_indices))):
-            for module in self.kv_modules:
-                # update the key/value cache to contain the selected sequences
-                self.kv_cache[module] = self.kv_cache[module][source_indices].detach()
-
+            blocks = self.cache_dyn.shape[0]
+            for i in range(blocks):
+                for j in range(2): # k and v 2 items
+                    self.cache_dyn[i][j] = self.cache_dyn[i][j][source_indices]
 
 class SequenceRanker:
     def rank(
diff --git a/whisper/model.py b/whisper/model.py
index a678283..c94a024 100644
--- a/whisper/model.py
+++ b/whisper/model.py
@@ -1,12 +1,14 @@
 import base64
 import gzip
 from dataclasses import dataclass
+import os
 from typing import Dict, Iterable, Optional
 
 import numpy as np
 import torch
 import torch.nn.functional as F
 from torch import Tensor, nn
+import mindietorch
 
 from .decoding import decode as decode_function
 from .decoding import detect_language as detect_language_function
@@ -153,24 +155,19 @@ class AudioEncoder(nn.Module):
             [ResidualAttentionBlock(n_state, n_head) for _ in range(n_layer)]
         )
         self.ln_post = LayerNorm(n_state)
+        self.device = "npu:0"
+        self.mindietorch_encoder_model = torch.jit.load(
+            "/tmp/models/encoder_compiled.ts"
+        ).eval().to(self.device)
 
     def forward(self, x: Tensor):
         """
         x : torch.Tensor, shape = (batch_size, n_mels, n_ctx)
             the mel spectrogram of the audio
         """
-        x = F.gelu(self.conv1(x))
-        x = F.gelu(self.conv2(x))
-        x = x.permute(0, 2, 1)
-
-        assert x.shape[1:] == self.positional_embedding.shape, "incorrect audio shape"
-        x = (x + self.positional_embedding).to(x.dtype)
-
-        for block in self.blocks:
-            x = block(x)
-
-        x = self.ln_post(x)
-        return x
+        x = x.to(self.device)
+        x = self.mindietorch_encoder_model(x)
+        return x.cpu()
 
 
 class TextDecoder(nn.Module):
@@ -193,29 +190,58 @@ class TextDecoder(nn.Module):
         mask = torch.empty(n_ctx, n_ctx).fill_(-np.inf).triu_(1)
         self.register_buffer("mask", mask, persistent=False)
 
-    def forward(self, x: Tensor, xa: Tensor, kv_cache: Optional[dict] = None):
-        """
-        x : torch.LongTensor, shape = (batch_size, <= n_ctx)
-            the text tokens
-        xa : torch.Tensor, shape = (batch_size, n_audio_ctx, n_audio_state)
-            the encoded audio features to be attended on
-        """
-        offset = next(iter(kv_cache.values())).shape[1] if kv_cache else 0
-        x = (
-            self.token_embedding(x)
-            + self.positional_embedding[offset : offset + x.shape[-1]]
-        )
-        x = x.to(xa.dtype)
-
-        for block in self.blocks:
-            x = block(x, xa, mask=self.mask, kv_cache=kv_cache)
+        self.device = "npu:0"
+        self.mindietorch_language_detection_model = torch.jit.load(
+            "/tmp/models/language_detection_compiled.ts"
+        ).eval().to(self.device)
+        self.mindietorch_prefill_model = torch.jit.load(
+            "/tmp/models/decoder_prefill_compiled.ts"
+        ).eval().to(self.device)
+        self.mindietorch_decode_model = torch.jit.load(
+            "/tmp/models/decoder_decode_compiled.ts"
+        ).eval().to(self.device)
 
-        x = self.ln(x)
-        logits = (
-            x @ torch.transpose(self.token_embedding.weight.to(x.dtype), 0, 1)
-        ).float()
-
-        return logits
+    def forward(
+        self,
+        x: Tensor,
+        xa: Tensor,
+        pos_embed: Tensor = None,
+        cache_dyn: Tensor = None,
+        cache_sta: Tensor = None,
+    ):
+        if cache_dyn is None:
+            tokens_npu = x.float().to(self.device)
+            audio_features_npu = xa.to(self.device)
+            pos_embed_npu = pos_embed.to(self.device)
+            if x.shape[0] != 1:
+                logits, cache_dyn, cache_sta = self.mindietorch_prefill_model(
+                    tokens_npu,
+                    audio_features_npu,
+                    pos_embed_npu
+                )
+            else:
+                logits, cache_dyn, cache_sta = self.mindietorch_language_detection_model(
+                    tokens_npu,
+                    audio_features_npu,
+                    pos_embed_npu
+                )
+            logits = logits.cpu()
+            cache_dyn = cache_dyn.cpu()
+        else:
+            tokens_npu = x.float().to(self.device)
+            audio_features_npu = xa.to(self.device)
+            pos_embed_npu = pos_embed.to(self.device)
+            cache_dyn_npu = cache_dyn.to(self.device)
+            logits, cache_dyn, _ = self.mindietorch_decode_model(
+                tokens_npu,
+                audio_features_npu,
+                pos_embed_npu,
+                cache_dyn_npu,
+                cache_sta
+            )
+            logits = logits.cpu()
+            cache_dyn = cache_dyn.cpu()
+        return logits, cache_dyn, cache_sta
 
 
 class Whisper(nn.Module):
@@ -257,7 +283,8 @@ class Whisper(nn.Module):
         return self.encoder(mel)
 
     def logits(self, tokens: torch.Tensor, audio_features: torch.Tensor):
-        return self.decoder(tokens, audio_features)
+        pos_embed = self.decoder.positional_embedding[:tokens.shape[-1]]
+        return self.decoder(tokens, audio_features, pos_embed)
 
     def forward(
         self, mel: torch.Tensor, tokens: torch.Tensor
