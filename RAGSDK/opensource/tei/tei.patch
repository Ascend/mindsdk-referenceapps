diff --git a/backends/grpc-client/src/client.rs b/backends/grpc-client/src/client.rs
index 2f4868f..98e9cca 100644
--- a/backends/grpc-client/src/client.rs
+++ b/backends/grpc-client/src/client.rs
@@ -18,7 +18,7 @@ impl Client {
         let channel = Channel::builder(uri).connect().await?;
 
         Ok(Self {
-            stub: EmbeddingServiceClient::new(channel),
+            stub: EmbeddingServiceClient::new(channel).max_decoding_message_size(100*1024*1024).max_encoding_message_size(100*1024*1024),
         })
     }
 
@@ -32,7 +32,7 @@ impl Client {
             .await?;
 
         Ok(Self {
-            stub: EmbeddingServiceClient::new(channel),
+            stub: EmbeddingServiceClient::new(channel).max_decoding_message_size(100*1024*1024).max_encoding_message_size(100*1024*1024),
         })
     }
 
@@ -65,6 +65,27 @@ impl Client {
         Ok(response.embeddings)
     }
 
+    #[instrument(skip_all)]
+    pub async fn embed_all(
+        &mut self,
+        input_ids: Vec<u32>,
+        token_type_ids: Vec<u32>,
+        position_ids: Vec<u32>,
+        cu_seq_lengths: Vec<u32>,
+        max_length: u32,
+    ) -> Result<Vec<TokenEmbedding>> {
+        let request = tonic::Request::new(EmbedRequest {
+            input_ids,
+            token_type_ids,
+            position_ids,
+            max_length,
+            cu_seq_lengths,
+        })
+        .inject_context();
+        let response = self.stub.embed_all(request).await?.into_inner();
+        Ok(response.allembeddings)
+    }
+    
     #[instrument(skip_all)]
     pub async fn predict(
         &mut self,
diff --git a/backends/proto/embed.proto b/backends/proto/embed.proto
index 036f3db..71d72e0 100644
--- a/backends/proto/embed.proto
+++ b/backends/proto/embed.proto
@@ -5,6 +5,7 @@ package embedding.v1;
 service EmbeddingService {
     /// Decode token for a list of prefilled batches
     rpc Embed (EmbedRequest) returns (EmbedResponse);
+    rpc Embed_all (EmbedRequest) returns (RawEmbedResponse);
     /// Health check
     rpc Health (HealthRequest) returns (HealthResponse);
     /// Predict
@@ -38,3 +39,11 @@ message Score {
 message PredictResponse {
     repeated Score scores = 1;
 }
+
+message TokenEmbedding {
+    repeated Embedding embeddings = 1;
+}
+
+message RawEmbedResponse {
+    repeated TokenEmbedding allembeddings = 1;
+}
\ No newline at end of file
diff --git a/backends/python/server/Makefile b/backends/python/server/Makefile
index 6402d63..8ad0028 100644
--- a/backends/python/server/Makefile
+++ b/backends/python/server/Makefile
@@ -1,9 +1,3 @@
-include Makefile-flash-att
-include Makefile-flash-att-v2
-
-unit-tests:
-	pytest -s -vv -m "not private" tests
-
 gen-server:
 	# Compile protos
 	pip install grpcio-tools==1.62.2 mypy-protobuf==3.6.0 'types-protobuf' --no-cache-dir
diff --git a/backends/python/server/requirements.txt b/backends/python/server/requirements.txt
index 687ec10..e46f148 100644
--- a/backends/python/server/requirements.txt
+++ b/backends/python/server/requirements.txt
@@ -6,10 +6,10 @@ deprecated==1.2.15 ; python_version >= "3.9" and python_version < "3.13"
 filelock==3.16.1 ; python_version >= "3.9" and python_version < "3.13"
 fsspec==2024.10.0 ; python_version >= "3.9" and python_version < "3.13"
 googleapis-common-protos==1.66.0 ; python_version >= "3.9" and python_version < "3.13"
-grpc-interceptor==0.15.4 ; python_version >= "3.9" and python_version < "3.13"
-grpcio-reflection==1.62.3 ; python_version >= "3.9" and python_version < "3.13"
-grpcio-status==1.62.3 ; python_version >= "3.9" and python_version < "3.13"
-grpcio==1.68.0 ; python_version >= "3.9" and python_version < "3.13"
+grpc-interceptor==0.15.3 ; python_version >= "3.9" and python_version < "3.13"
+grpcio-reflection==1.58.0 ; python_version >= "3.9" and python_version < "3.13"
+grpcio-status==1.58.0 ; python_version >= "3.9" and python_version < "3.13"
+grpcio==1.58.0 ; python_version >= "3.9" and python_version < "3.13"
 huggingface-hub==0.26.2 ; python_version >= "3.9" and python_version < "3.13"
 idna==3.10 ; python_version >= "3.9" and python_version < "3.13"
 importlib-metadata==7.1.0 ; python_version >= "3.9" and python_version < "3.13"
@@ -19,7 +19,7 @@ loguru==0.6.0 ; python_version >= "3.9" and python_version < "3.13"
 markupsafe==3.0.2 ; python_version >= "3.9" and python_version < "3.13"
 mpmath==1.3.0 ; python_version >= "3.9" and python_version < "3.13"
 networkx==3.2.1 ; python_version >= "3.9" and python_version < "3.13"
-numpy==2.0.2 ; python_version >= "3.9" and python_version < "3.13"
+numpy==1.24.0 ; python_version >= "3.9" and python_version < "3.13"
 nvidia-cublas-cu12==12.4.5.8 ; platform_system == "Linux" and platform_machine == "x86_64" and python_version >= "3.9" and python_version < "3.13"
 nvidia-cuda-cupti-cu12==12.4.127 ; platform_system == "Linux" and platform_machine == "x86_64" and python_version >= "3.9" and python_version < "3.13"
 nvidia-cuda-nvrtc-cu12==12.4.127 ; platform_system == "Linux" and platform_machine == "x86_64" and python_version >= "3.9" and python_version < "3.13"
@@ -51,14 +51,14 @@ requests==2.32.3 ; python_version >= "3.9" and python_version < "3.13"
 safetensors==0.4.5 ; python_version >= "3.9" and python_version < "3.13"
 scikit-learn==1.5.2 ; python_version >= "3.9" and python_version < "3.13"
 scipy==1.13.1 ; python_version >= "3.9" and python_version < "3.13"
-sentence-transformers==3.3.1 ; python_version >= "3.9" and python_version < "3.13"
+sentence-transformers==3.4.1 ; python_version >= "3.9" and python_version < "3.13"
 setuptools==75.6.0 ; python_version >= "3.9" and python_version < "3.13"
 sympy==1.13.1 ; python_version >= "3.9" and python_version < "3.13"
 threadpoolctl==3.5.0 ; python_version >= "3.9" and python_version < "3.13"
 tokenizers==0.20.3 ; python_version >= "3.9" and python_version < "3.13"
-torch==2.5.1 ; python_version >= "3.9" and python_version < "3.13"
+torch==2.1.0 ; python_version >= "3.9" and python_version < "3.13"
 tqdm==4.67.1 ; python_version >= "3.9" and python_version < "3.13"
-transformers==4.46.3 ; python_version >= "3.9" and python_version < "3.13"
+transformers==4.51.3 ; python_version >= "3.9" and python_version < "3.13"
 triton==3.1.0 ; platform_system == "Linux" and platform_machine == "x86_64" and python_version < "3.13" and python_version >= "3.9"
 typer==0.6.1 ; python_version >= "3.9" and python_version < "3.13"
 typing-extensions==4.12.2 ; python_version >= "3.9" and python_version < "3.13"
diff --git a/backends/python/server/text_embeddings_server/models/__init__.py b/backends/python/server/text_embeddings_server/models/__init__.py
index 1e919f2..90d9487 100644
--- a/backends/python/server/text_embeddings_server/models/__init__.py
+++ b/backends/python/server/text_embeddings_server/models/__init__.py
@@ -11,31 +11,21 @@ from text_embeddings_server.models.model import Model
 from text_embeddings_server.models.masked_model import MaskedLanguageModel
 from text_embeddings_server.models.default_model import DefaultModel
 from text_embeddings_server.models.classification_model import ClassificationModel
-from text_embeddings_server.models.jinaBert_model import FlashJinaBert
-from text_embeddings_server.models.flash_mistral import FlashMistral
-from text_embeddings_server.models.flash_qwen3 import FlashQwen3
+from text_embeddings_server.models.qwen3_rerank_model import Qwen3RerankModel
+from text_embeddings_server.models.unixcoder_model import UniXcoderModel
+
 from text_embeddings_server.utils.device import get_device, use_ipex
 
+from modeling_bert_adapter import enable_bert_speed
+from modeling_roberta_adapter import enable_roberta_speed
+from modeling_xlm_roberta_adapter import enable_xlm_roberta_speed
+
 __all__ = ["Model"]
 
 TRUST_REMOTE_CODE = os.getenv("TRUST_REMOTE_CODE", "false").lower() in ["true", "1"]
-DISABLE_TENSOR_CACHE = os.getenv("DISABLE_TENSOR_CACHE", "false").lower() in [
-    "true",
-    "1",
-]
 # Disable gradients
 torch.set_grad_enabled(False)
 
-FLASH_ATTENTION = True
-try:
-    from text_embeddings_server.models.flash_bert import FlashBert
-except ImportError as e:
-    logger.warning(f"Could not import Flash Attention enabled models: {e}")
-    FLASH_ATTENTION = False
-
-if FLASH_ATTENTION:
-    __all__.append(FlashBert)
-
 
 def wrap_model_if_hpu(model_handle, device):
     """Wrap the model in HPU graph if the device is HPU."""
@@ -70,22 +60,21 @@ def get_model(model_path: Path, dtype: Optional[str], pool: str):
     else:
         raise RuntimeError(f"Unknown dtype {dtype}")
 
+
+    enable_boost = os.getenv("ENABLE_BOOST", "False")
+    if enable_boost not in("True", "False"):
+        raise ValueError("env ENABLE_BOOST value must be True or False")
+    
+    if enable_boost == "True":
+        dtype == torch.float16
+        
     device = get_device()
     logger.info(f"backend device: {device}")
 
     config = AutoConfig.from_pretrained(model_path, trust_remote_code=TRUST_REMOTE_CODE)
 
-    if (
-        hasattr(config, "auto_map")
-        and isinstance(config.auto_map, dict)
-        and "AutoModel" in config.auto_map
-        and config.auto_map["AutoModel"]
-        == "jinaai/jina-bert-v2-qk-post-norm--modeling_bert.JinaBertModel"
-    ):
-        # Add specific offline modeling for model "jinaai/jina-embeddings-v2-base-code" which uses "autoMap" to reference code in other repository
-        return create_model(FlashJinaBert, model_path, device, datatype)
-
-    if config.model_type == "bert":
+    if config.model_type == "bert" or config.model_type == "qwen2" or config.model_type == "qwen3" or \
+        config.model_type == "roberta" or config.model_type == "xlm-roberta":
         config: BertConfig
         if (
             use_ipex()
@@ -111,8 +100,12 @@ def get_model(model_path: Path, dtype: Optional[str], pool: str):
 
         if config.architectures[0].endswith("Classification"):
             return create_model(ClassificationModel, model_path, device, datatype)
+        elif os.getenv("IS_RERANK", None):
+            return create_model(Qwen3RerankModel, model_path, device, datatype)
         elif config.architectures[0].endswith("ForMaskedLM") and pool == "splade":
             return create_model(MaskedLanguageModel, model_path, device, datatype)
+        elif str(model_path).endswith("unixcoder-base"):
+            return create_model(UniXcoderModel, model_path, device, datatype)
         else:
             return create_model(DefaultModel, model_path, device, datatype, pool)
 
diff --git a/backends/python/server/text_embeddings_server/models/classification_model.py b/backends/python/server/text_embeddings_server/models/classification_model.py
index 91f33ed..a1395ef 100644
--- a/backends/python/server/text_embeddings_server/models/classification_model.py
+++ b/backends/python/server/text_embeddings_server/models/classification_model.py
@@ -70,3 +70,4 @@ class ClassificationModel(Model):
         output = self.model(**kwargs, return_dict=True)
         all_scores = output.logits.tolist()
         return [Score(values=scores) for scores in all_scores]
+
diff --git a/backends/python/server/text_embeddings_server/models/default_model.py b/backends/python/server/text_embeddings_server/models/default_model.py
index 66a9b6b..c9385e3 100644
--- a/backends/python/server/text_embeddings_server/models/default_model.py
+++ b/backends/python/server/text_embeddings_server/models/default_model.py
@@ -1,17 +1,24 @@
+import os
 import inspect
 import torch
 
 from pathlib import Path
 from typing import Type, List
-from transformers import AutoModel
+from transformers import AutoModel, AutoConfig, AutoTokenizer
 from opentelemetry import trace
+
+from collections import defaultdict
+import numpy as np
+from loguru import logger
+
 from text_embeddings_server.models.pooling import DefaultPooling
 
 from text_embeddings_server.models import Model
-from text_embeddings_server.models.types import PaddedBatch, Embedding, Score
+from text_embeddings_server.models.types import PaddedBatch, Embedding, Score, TokenEmbedding
 
 tracer = trace.get_tracer(__name__)
 
+IS_CAUSAL = os.getenv("IS_CAUSAL", "").lower()
 
 class DefaultModel(Model):
     def __init__(
@@ -19,16 +26,18 @@ class DefaultModel(Model):
         model_path: Path,
         device: torch.device,
         dtype: torch.dtype,
-        pool: str = "cls",
+        pool: str,
         trust_remote: bool = False,
     ):
         model = (
             AutoModel.from_pretrained(model_path, trust_remote_code=trust_remote)
             .to(dtype)
             .to(device)
+            .eval()
         )
         self.hidden_size = model.config.hidden_size
-        self.pooling = DefaultPooling(self.hidden_size, pooling_mode=pool)
+        
+        self.pool = pool
 
         position_offset = 0
         model_type = model.config.model_type
@@ -50,6 +59,18 @@ class DefaultModel(Model):
             is not None
         )
 
+        if self.pool == "splade":
+            self.tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
+            self.config = AutoConfig.from_pretrained(model_path)
+            self.vocab_size = self.config.vocab_size
+        
+            self.sparse_linear = torch.nn.Linear(self.hidden_size, 1).to(device).to(dtype)
+            sparse_model_path = os.path.join(model_path, "sparse_linear.pt")
+            sparse_state_dict = torch.load(sparse_model_path, map_location="cpu", weights_only=True)
+            self.sparse_linear.load_state_dict(sparse_state_dict)
+        else:
+            self.pooling = DefaultPooling(self.hidden_size, pooling_mode=pool)
+                
         super(DefaultModel, self).__init__(model=model, dtype=dtype, device=device)
 
     @property
@@ -63,19 +84,76 @@ class DefaultModel(Model):
             kwargs["token_type_ids"] = batch.token_type_ids
         if self.has_position_ids:
             kwargs["position_ids"] = batch.position_ids
-        output = self.model(**kwargs)
 
-        embedding = self.pooling.forward(output, batch.attention_mask)
+        if self.pool == "splade":
+            return self._process_splade(batch, kwargs)
+        else:
+            return self._process_default(batch, kwargs)
 
+    def _process_splade(self, batch: PaddedBatch, kwargs: dict):
+        with torch.no_grad():
+            last_hidden_state = self.model(**kwargs, return_dict=True).last_hidden_state
+        sparse_vecs = torch.relu(self.sparse_linear(last_hidden_state))
+        token_weights = sparse_vecs.squeeze(-1)
+        unused_tokens = {self.tokenizer.cls_token_id, self.tokenizer.eos_token_id, self.tokenizer.pad_token_id,
+                         self.tokenizer.unk_token_id}
+        all_token_weights = torch.zeros((len(batch), self.vocab_size), dtype=token_weights.dtype, device=token_weights.device)
+        
+        for i in range(len(batch)):
+            all_token_weights[i, batch.input_ids[i]] = token_weights[i]
+        
+        all_token_weights[:, list(unused_tokens)] = 0
+        
+        embeddings = [
+            Embedding(
+                values=all_token_weights[i].detach().cpu().numpy().tolist()
+            )
+            for i in range(len(batch))
+        ]
+        
+        return embeddings
+    
+    def _process_default(self, batch: PaddedBatch, kwargs: dict):
+        if IS_CAUSAL in ["false", "0"]:
+            with torch.no_grad():
+                output = self.model(**kwargs, is_causal=False)
+        else:
+            with torch.no_grad():
+                output = self.model(**kwargs)
+            
+        embedding = self.pooling.forward(output, batch.attention_mask)
         cpu_results = embedding.view(-1).tolist()
-
+        
         return [
             Embedding(
                 values=cpu_results[i * self.hidden_size : (i + 1) * self.hidden_size]
             )
             for i in range(len(batch))
         ]
+    
+    @tracer.start_as_current_span("embed_all")
+    def embed_all(self, batch: PaddedBatch):
+        kwargs = {"input_ids": batch.input_ids, "attention_mask": batch.attention_mask}
+        if self.has_token_type_ids:
+            kwargs["token_type_ids"] = batch.token_type_ids
+        if self.has_position_ids:
+            kwargs["position_ids"] = batch.position_ids
+        output = self.model(**kwargs)
+        embedding = output[0].contiguous()
+        cpu_results = embedding.view(-1).tolist()
+        embedding_result=[]
+        for i in range(len(batch)):
+            embedding_tmp=[
+                Embedding(values=cpu_results[(j+i * batch.max_length) * self.hidden_size :
+                                             (j + 1 + i * batch.max_length) * self.hidden_size])
+                for j in range(batch.input_ids.size()[1])
+            ]
+            token_embeddings=TokenEmbedding(embeddings=embedding_tmp)
+            embedding_result.append(token_embeddings)
 
+        return embedding_result
+    
     @tracer.start_as_current_span("predict")
     def predict(self, batch: PaddedBatch) -> List[Score]:
         pass
+
diff --git a/backends/python/server/text_embeddings_server/models/pooling.py b/backends/python/server/text_embeddings_server/models/pooling.py
index 43f77b1..69c69f9 100644
--- a/backends/python/server/text_embeddings_server/models/pooling.py
+++ b/backends/python/server/text_embeddings_server/models/pooling.py
@@ -25,7 +25,7 @@ class DefaultPooling(_Pooling):
     def forward(self, model_output, attention_mask) -> Tensor:
         pooling_features = {
             "token_embeddings": model_output[0],
-            "attention_mask": attention_mask,
+            "attention_mask": attention_mask.type(model_output[0].dtype),
         }
         return self.pooling.forward(pooling_features)["sentence_embedding"]
 
diff --git a/backends/python/server/text_embeddings_server/models/qwen3_rerank_model.py b/backends/python/server/text_embeddings_server/models/qwen3_rerank_model.py
new file mode 100644
index 0000000..fb009f6
--- /dev/null
+++ b/backends/python/server/text_embeddings_server/models/qwen3_rerank_model.py
@@ -0,0 +1,97 @@
+import inspect
+import torch
+import os
+
+from pathlib import Path
+from typing import Type, List
+from transformers import AutoModelForSequenceClassification, Qwen3ForCausalLM, AutoTokenizer, AutoModelForCausalLM
+from opentelemetry import trace
+
+from text_embeddings_server.models import Model
+from text_embeddings_server.models.types import PaddedBatch, Embedding, Score
+
+tracer = trace.get_tracer(__name__)
+
+
+class Qwen3RerankModel(Model):
+    def __init__(
+        self,
+        model_path: Path,
+        device: torch.device,
+        dtype: torch.dtype,
+        pool: str = "cls",
+        trust_remote: bool = False,
+    ):
+
+        # Check environment variable to decide reranker mode
+        self.qwen3_mode = os.environ.get("IS_RERANK", "0") == "1"
+        position_offset = 0
+        # Load tokenizer (for both modes)
+        self.tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side="left",
+                                                        trust_remote_code=trust_remote)
+        # 为 Qwen3 模型设置 pad_token 以支持 batch 推理
+        if self.tokenizer.pad_token is None:
+            self.tokenizer.pad_token = self.tokenizer.eos_token
+        # -------------------------------
+        #  Qwen3-Reranker 初始化逻辑
+        # -------------------------------
+        self.model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=trust_remote)
+        self.model = self.model.to(dtype).to(device).eval()
+        self.model.config.pad_token_id = self.tokenizer.pad_token_id
+        # 用于从 logits 提取 "yes" 和 "no" 的得分
+        self.token_true_id = self.tokenizer.convert_tokens_to_ids("yes")
+        self.token_false_id = self.tokenizer.convert_tokens_to_ids("no")
+        prefix = "<|im_start|>system\nDetermine whether the problem is related to the document. Note that the answer can only be \"yes\" or \"no\".<|im_end|>\n<|im_start|>user\n"
+        suffix = "<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n"
+        self.prefix_tokens = self.tokenizer.encode(prefix, add_special_tokens=False)
+        self.suffix_tokens = self.tokenizer.encode(suffix, add_special_tokens=False)
+
+        if hasattr(self.model.config, "max_seq_length"):
+            self.max_input_length = self.model.config.max_seq_length
+        else:
+            self.max_input_length = (
+                    self.model.config.max_position_embeddings - position_offset
+            )
+
+        super(Qwen3RerankModel, self).__init__(
+            model=self.model, dtype=dtype, device=device
+        )
+
+
+    @property
+    def batch_type(self) -> Type[PaddedBatch]:
+        return PaddedBatch
+
+    @tracer.start_as_current_span("embed")
+    def embed(self, batch: PaddedBatch) -> List[Embedding]:
+        pass
+
+    @tracer.start_as_current_span("predict")
+    def predict(self, batch: PaddedBatch) -> List[Score]:
+        kwargs = {"input_ids": batch.input_ids, "attention_mask": batch.attention_mask}
+        # Qwen3-Reranker 的打分逻辑（prompt 生成 + yes/no logits）
+        input_ids = []
+        for ele in batch.input_ids:
+            ids = self.prefix_tokens + ele.tolist() + self.suffix_tokens
+            input_ids.append(ids)
+
+        # 注意：传入的是 list 而非 tensor
+        tokenized = self.tokenizer.pad(
+            {"input_ids": input_ids},
+            padding=True,
+            return_tensors="pt",
+            max_length=self.max_input_length
+         )
+        inputs = {k: v.to(self.model.device) for k, v in tokenized.items()}
+
+        with torch.no_grad():
+            outputs = self.model(**inputs)
+            logits = outputs.logits[:, -1, :]  # 最后一个 token 的预测分布
+
+            # 提取 "yes" 和 "no" 的概率，作为是否相关的判断
+            true_logits = logits[:, self.token_true_id]
+            false_logits = logits[:, self.token_false_id]
+            logit_diff = true_logits - false_logits
+        return [Score(values=[p.item()]) for p in logit_diff]
+
+
diff --git a/backends/python/server/text_embeddings_server/models/types.py b/backends/python/server/text_embeddings_server/models/types.py
index f27572a..71db970 100644
--- a/backends/python/server/text_embeddings_server/models/types.py
+++ b/backends/python/server/text_embeddings_server/models/types.py
@@ -7,7 +7,7 @@ from dataclasses import dataclass
 from opentelemetry import trace
 
 from text_embeddings_server.pb import embed_pb2
-from text_embeddings_server.pb.embed_pb2 import Embedding, Score
+from text_embeddings_server.pb.embed_pb2 import Embedding, Score, TokenEmbedding
 
 tracer = trace.get_tracer(__name__)
 PAD_SEQUENCE_TO_MULTIPLE_OF = int(os.environ.get("PAD_SEQUENCE_TO_MULTIPLE_OF", 128))
@@ -36,6 +36,7 @@ class PaddedBatch(Batch):
     token_type_ids: torch.Tensor
     position_ids: torch.Tensor
     attention_mask: torch.Tensor
+    max_length: int
 
     @classmethod
     @tracer.start_as_current_span("from_pb")
@@ -82,6 +83,7 @@ class PaddedBatch(Batch):
             token_type_ids=all_tensors[1],
             position_ids=all_tensors[2],
             attention_mask=all_tensors[3],
+            max_length=max_length,
         )
 
     def __len__(self):
diff --git a/backends/python/server/text_embeddings_server/models/unixcoder_model.py b/backends/python/server/text_embeddings_server/models/unixcoder_model.py
new file mode 100644
index 0000000..ea95871
--- /dev/null
+++ b/backends/python/server/text_embeddings_server/models/unixcoder_model.py
@@ -0,0 +1,88 @@
+import os
+import inspect
+import torch
+
+from pathlib import Path
+from typing import Type, List
+from transformers import RobertaModel, RobertaConfig, RobertaTokenizer
+from opentelemetry import trace
+
+from collections import defaultdict
+import numpy as np
+from loguru import logger
+
+from text_embeddings_server.models.pooling import DefaultPooling
+
+from text_embeddings_server.models import Model
+from text_embeddings_server.models.types import PaddedBatch, Embedding, Score, TokenEmbedding
+
+tracer = trace.get_tracer(__name__)
+
+is_unixcode = os.getenv("IS_UNIXCODE", None)
+
+class UniXcoderModel(Model):
+    def __init__(
+        self,
+        model_path: Path,
+        device: torch.device,
+        dtype: torch.dtype,
+        pool: str,
+        trust_remote: bool = False,
+    ):
+        self.config = RobertaConfig.from_pretrained(model_path)
+        self.config.is_decoder = True
+        model = (
+            RobertaModel.from_pretrained(model_path, config=self.config)
+            .to(dtype)
+            .to(device)
+            .eval()
+        )
+
+        self.hidden_size = model.config.hidden_size 
+
+        position_offset = 0
+        model_type = model.config.model_type
+        if model_type in ["xlm-roberta", "camembert", "roberta"]:
+            position_offset = model.config.pad_token_id + 1
+        if hasattr(model.config, "max_seq_length"):
+            self.max_input_length = model.config.max_seq_length
+        else:
+            self.max_input_length = (
+                model.config.max_position_embeddings - position_offset
+            )
+
+        self.tokenizer = RobertaTokenizer.from_pretrained(model_path, local_files_only=True)
+                
+        super(UniXcoderModel, self).__init__(model=model, dtype=dtype, device=device)
+
+    @property
+    def batch_type(self) -> Type[PaddedBatch]:
+        return PaddedBatch
+
+    @tracer.start_as_current_span("embed")
+    def embed(self, batch: PaddedBatch) -> List[Embedding]:
+        kwargs = {"input_ids": batch.input_ids, "attention_mask": batch.attention_mask}
+        return self._process_unixcode(batch, kwargs)
+
+    def _process_unixcode(self, batch: PaddedBatch, kwargs: dict):
+        tokens_ids = []
+        mode = "<encoder-only>"
+        mode_id = self.tokenizer.convert_tokens_to_ids(mode)
+        for tokens_id in batch.input_ids:
+            tokens_id = tokens_id[:self.max_input_length - 4]
+            tokens_id = tokens_id.tolist()[1:-1]
+            tokens_id = [self.tokenizer.cls_token_id, mode_id, self.tokenizer.sep_token_id] + tokens_id + [self.tokenizer.sep_token_id]
+
+            tokens_ids.append(tokens_id)
+        tokens_ids = torch.tensor(tokens_ids).to(self.device)
+        mask = tokens_ids.ne(self.config.pad_token_id)
+        attention_mask=mask.unsqueeze(1) * mask.unsqueeze(2).to(self.device)
+        token_embeddings = self.model(tokens_ids, attention_mask=attention_mask)[0]
+        sentence_embeddings = (token_embeddings * mask.unsqueeze(-1)).sum(1) / mask.sum(-1).unsqueeze(-1)
+        return [
+            Embedding(
+                values=sentence_embeddings[i]
+            )
+            for i in range(len(batch))
+        ]
+
diff --git a/backends/python/server/text_embeddings_server/server.py b/backends/python/server/text_embeddings_server/server.py
index 646d79b..da8f6a1 100644
--- a/backends/python/server/text_embeddings_server/server.py
+++ b/backends/python/server/text_embeddings_server/server.py
@@ -1,5 +1,8 @@
 import asyncio
 import torch
+import torch_npu
+import os
+
 from grpc import aio
 from loguru import logger
 
@@ -13,6 +16,9 @@ from text_embeddings_server.utils.tracing import UDSOpenTelemetryAioServerInterc
 from text_embeddings_server.utils.interceptor import ExceptionInterceptor
 
 
+clean_npu_cache = os.getenv("CLEAN_NPU_CACHE", "False")
+
+
 class EmbeddingService(embed_pb2_grpc.EmbeddingServiceServicer):
     def __init__(self, model: Model):
         self.model = model
@@ -31,9 +37,22 @@ class EmbeddingService(embed_pb2_grpc.EmbeddingServiceServicer):
         )
 
         embeddings = self.model.embed(batch)
+        if clean_npu_cache == "True":
+            torch_npu.npu.empty_cache()
 
         return embed_pb2.EmbedResponse(embeddings=embeddings)
 
+    async def Embed_all(self, request, context):
+        max_input_length = self.model.max_input_length
+        batch = self.model.batch_type.from_pb(request, self.model.device, max_input_length)
+
+        embeddings = self.model.embed_all(batch)
+        
+        if clean_npu_cache == "True":
+            torch_npu.npu.empty_cache()
+            
+        return embed_pb2.RawEmbedResponse(allembeddings=embeddings)
+    
     async def Predict(self, request, context):
         max_input_length = self.model.max_input_length
         batch = self.model.batch_type.from_pb(
@@ -42,6 +61,9 @@ class EmbeddingService(embed_pb2_grpc.EmbeddingServiceServicer):
 
         scores = self.model.predict(batch)
 
+        if clean_npu_cache == "True":
+            torch_npu.npu.empty_cache()
+            
         return embed_pb2.PredictResponse(scores=scores)
 
 
@@ -67,6 +89,10 @@ def serve(
             interceptors=[
                 ExceptionInterceptor(),
                 UDSOpenTelemetryAioServerInterceptor(),
+            ],
+            options = [
+                ('grpc_max_send_message_length', 100 * 1024 * 1024),
+                ('grpc_max_recieve_message_length', 100 * 1024 * 1024),
             ]
         )
         embed_pb2_grpc.add_EmbeddingServiceServicer_to_server(
diff --git a/backends/python/server/text_embeddings_server/utils/device.py b/backends/python/server/text_embeddings_server/utils/device.py
index 3f3b04d..04a25b3 100644
--- a/backends/python/server/text_embeddings_server/utils/device.py
+++ b/backends/python/server/text_embeddings_server/utils/device.py
@@ -4,6 +4,7 @@ import importlib.metadata
 import importlib.util
 from packaging import version
 import torch
+import torch_npu
 import subprocess
 
 ALLOW_REDUCED_PRECISION = os.getenv(
@@ -59,6 +60,14 @@ def get_device():
     device = torch.device("cpu")
     if torch.cuda.is_available():
         device = torch.device("cuda")
+    elif torch.npu.is_available():
+        device = torch.device("npu")
+        torch.npu.set_compile_mode(jit_compile=False)
+        option = {"NPU_FUZZY_COMPILE_BLACKLIST": "ReduceProd"}
+        torch.npu.set_option(option)
+        deviceIdx = os.environ.get('TEI_NPU_DEVICE')
+        if deviceIdx != None and deviceIdx.isdigit() and int(deviceIdx) >= 0 and int(deviceIdx) <= 7:
+            torch.npu.set_device(torch.device(f"npu:{deviceIdx}"))
     elif is_hpu():
         import habana_frameworks.torch.core as htcore
 
diff --git a/backends/python/src/lib.rs b/backends/python/src/lib.rs
index 53255b0..4d24016 100644
--- a/backends/python/src/lib.rs
+++ b/backends/python/src/lib.rs
@@ -73,31 +73,53 @@ impl Backend for PythonBackend {
     }
 
     fn embed(&self, batch: Batch) -> Result<Embeddings, BackendError> {
-        if !batch.raw_indices.is_empty() {
-            return Err(BackendError::Inference(
-                "raw embeddings are not supported for the Python backend.".to_string(),
-            ));
-        }
         let batch_size = batch.len();
 
-        let results = self
-            .tokio_runtime
-            .block_on(self.backend_client.clone().embed(
-                batch.input_ids,
-                batch.token_type_ids,
-                batch.position_ids,
-                batch.cumulative_seq_lengths,
-                batch.max_length,
-            ))
-            .map_err(|err| BackendError::Inference(err.to_string()))?;
-        let pooled_embeddings: Vec<Vec<f32>> = results.into_iter().map(|r| r.values).collect();
-
         let mut embeddings =
             HashMap::with_capacity_and_hasher(batch_size, BuildNoHashHasher::default());
-        for (i, e) in pooled_embeddings.into_iter().enumerate() {
-            embeddings.insert(i, Embedding::Pooled(e));
-        }
 
+        if !batch.pooled_indices.is_empty() {
+            let results = self
+                .tokio_runtime
+                .block_on(self.backend_client.clone().embed(
+                    batch.input_ids,
+                    batch.token_type_ids,
+                    batch.position_ids,
+                    batch.cumulative_seq_lengths,
+                    batch.max_length,
+                ))
+                .map_err(|err| BackendError::Inference(err.to_string()))?;
+
+            let pooled_embeddings: Vec<Vec<f32>> = results.into_iter().map(|r| r.values).collect();
+            for (i, e) in pooled_embeddings.into_iter().enumerate() {
+                embeddings.insert(i, Embedding::Pooled(e));
+            }
+        }
+        else if !batch.raw_indices.is_empty() {
+            let results = self
+                .tokio_runtime
+                .block_on(self.backend_client.clone().embed_all(
+                    batch.input_ids,
+                    batch.token_type_ids,
+                    batch.position_ids,
+                    batch.cumulative_seq_lengths,
+                    batch.max_length,
+                ))
+                .map_err(|err| BackendError::Inference(err.to_string()))?;
+            
+            let mut raw_embeddings = Vec::new();
+            for token_embedding in results {
+                let mut two_dim_list = Vec::new();
+                for embeddings in token_embedding.embeddings {
+                    let values = embeddings.values.clone();
+                    two_dim_list.push(values);
+                }
+                raw_embeddings.push(two_dim_list);
+            }
+            for (i, e) in raw_embeddings.into_iter().enumerate() {
+                embeddings.insert(i, Embedding::All(e));
+            }
+        }
         Ok(embeddings)
     }
 
diff --git a/backends/src/dtype.rs b/backends/src/dtype.rs
index 80292be..253f322 100644
--- a/backends/src/dtype.rs
+++ b/backends/src/dtype.rs
@@ -53,7 +53,7 @@ impl Default for DType {
         }
         #[cfg(feature = "python")]
         {
-            DType::Bfloat16
+            DType::Float16
         }
     }
 }
diff --git a/core/src/queue.rs b/core/src/queue.rs
index 3fd8b77..61b38dc 100644
--- a/core/src/queue.rs
+++ b/core/src/queue.rs
@@ -153,6 +153,7 @@ fn queue_blocking_task(
                     };
 
                     if total_tokens > max_batch_tokens {
+                        tracing::info!("split new batch because cur up to max_batch_tokens:{max_batch_tokens:?}");
                         entries.push_front(entry);
                         break;
                     }
@@ -174,7 +175,8 @@ fn queue_blocking_task(
 
                     entry_index += 1;
 
-                    if Some(metadata.len()) == max_batch_requests {
+                    if Some(metadata.len()) == Some(capacity) {
+                        tracing::info!("split new batch because cur up to:{capacity:?}");
                         break;
                     }
                 }
@@ -183,6 +185,7 @@ fn queue_blocking_task(
                 let next_batch = if metadata.is_empty() {
                     None
                 } else {
+                    tracing::info!("inference batch size is:{batch_size:?}");
                     Some((
                         metadata,
                         Batch {
diff --git a/core/src/tokenization.rs b/core/src/tokenization.rs
index 7636afa..ab9a222 100644
--- a/core/src/tokenization.rs
+++ b/core/src/tokenization.rs
@@ -293,7 +293,7 @@ fn tokenize_input(
     prompts: Option<&HashMap<String, String>>,
     tokenizer: &mut Tokenizer,
 ) -> Result<(Option<String>, RawEncoding), TextEmbeddingsError> {
-    let pre_prompt = prepare_pre_prompt(default_prompt, prompt_name, prompts)?;
+    let pre_prompt = prepare_pre_prompt(default_prompt.clone(), prompt_name, prompts)?;
 
     let input_chars = inputs.count_chars();
     let limit = max_input_length * MAX_CHAR_MULTIPLIER;
@@ -322,20 +322,41 @@ fn tokenize_input(
 
             (Some(s), encoding)
         }
+
         EncodingInput::Dual(s1, s2) => {
-            if pre_prompt.is_some() {
+            let is_rerank = std::env::var("IS_RERANK").ok().as_deref() == Some("1");
+        
+            if is_rerank {
+                let default_prompt = default_prompt.ok_or_else(|| {
+                    TextEmbeddingsError::Validation(
+                        "In rerank mode, `--default-prompt` must be set.".to_string(),
+                    )
+                })?;
+        
+                let prompt = default_prompt
+                    .replace("\\n", "\n")
+                    .replace("<s1>", &s1)
+                    .replace("<s2>", &s2);
+        
+                let encoding = tokenizer
+                    .with_truncation(truncate_params)?
+                    .encode::<&str>(&prompt, add_special_tokens)?;
+        
+                (Some(prompt), encoding)
+            } else if pre_prompt.is_some() {
                 return Err(TextEmbeddingsError::Validation(
                     "`prompt_name` cannot be set with dual inputs".to_string(),
                 ));
+            } else {
+                (
+                    None,
+                    tokenizer
+                        .with_truncation(truncate_params)?
+                        .encode::<(String, String)>((s1, s2), add_special_tokens)?,
+                )
             }
-
-            (
-                None,
-                tokenizer
-                    .with_truncation(truncate_params)?
-                    .encode::<(String, String)>((s1, s2), add_special_tokens)?,
-            )
         }
+
         // input is encoded -> convert to tokenizers Encoding
         EncodingInput::Ids(ids) => {
             if let Some(mut pre_prompt) = pre_prompt {
diff --git a/router/src/http/server.rs b/router/src/http/server.rs
index f805744..1a5244a 100644
--- a/router/src/http/server.rs
+++ b/router/src/http/server.rs
@@ -7,7 +7,7 @@ use crate::http::types::{
     RerankRequest, RerankResponse, Sequence, SimilarityInput, SimilarityParameters,
     SimilarityRequest, SimilarityResponse, SimpleToken, SparseValue, TokenizeInput,
     TokenizeRequest, TokenizeResponse, TruncationDirection, VertexPrediction, VertexRequest,
-    VertexResponse,
+    VertexResponse, OpenAICompatRerankRequest, OpenAICompatRerankResponse, OpenAIRank, DocumentWrapper
 };
 use crate::{
     logging, shutdown, ClassifierModel, EmbeddingModel, ErrorResponse, ErrorType, Info, ModelType,
@@ -1296,6 +1296,207 @@ async fn openai_embed(
     Ok((headers, Json(response)))
 }
 
+
+/// Get Ranks. Returns a 424 status code if the model is not a Sequence Classification model with
+/// a single class.
+#[utoipa::path(
+    post,
+    tag = "Text Embeddings Inference",
+    path = "/v1/rerank",
+    request_body = OpenAICompatRerankRequest,
+    responses(
+    (status = 200, description = "Ranks", body = OpenAICompatRerankResponse),
+    (status = 424, description = "Rerank Error", body = ErrorResponse,
+    example = json ! ({"error": "Inference failed", "error_type": "backend"})),
+    (status = 429, description = "Model is overloaded", body = ErrorResponse,
+    example = json ! ({"error": "Model is overloaded", "error_type": "overloaded"})),
+    (status = 422, description = "Tokenization error", body = ErrorResponse,
+    example = json ! ({"error": "Tokenization error", "error_type": "tokenizer"})),
+    (status = 400, description = "Batch is empty", body = ErrorResponse,
+    example = json ! ({"error": "Batch is empty", "error_type": "empty"})),
+    (status = 413, description = "Batch size error", body = ErrorResponse,
+    example = json ! ({"error": "Batch size error", "error_type": "validation"})),
+    )
+    )]
+    #[instrument(
+        skip_all,
+        fields(total_time, tokenization_time, queue_time, inference_time,)
+    )]
+    async fn openai_rerank(
+        infer: Extension<Infer>,
+        info: Extension<Info>,
+        Extension(context): Extension<Option<opentelemetry::Context>>,
+        Json(req): Json<OpenAICompatRerankRequest>,
+    ) -> Result<(HeaderMap, Json<OpenAICompatRerankResponse>), (StatusCode, Json<ErrorResponse>)> {
+        let span = tracing::Span::current();
+        if let Some(context) = context {
+            span.set_parent(context);
+        }
+    
+        let start_time = Instant::now();
+    
+        if req.documents.is_empty() {
+            let message = "`documents` cannot be empty".to_string();
+            tracing::error!("{message}");
+            let err = ErrorResponse {
+                error: message,
+                error_type: ErrorType::Empty,
+            };
+            let counter = metrics::counter!("te_request_failure", "err" => "validation");
+            counter.increment(1);
+            Err(err)?;
+        }
+    
+        match &info.model_type {
+            ModelType::Reranker(_) => Ok(()),
+            ModelType::Classifier(_) | ModelType::Embedding(_) => {
+                let counter = metrics::counter!("te_request_failure", "err" => "model_type");
+                counter.increment(1);
+                let message = "model is not a re-ranker model".to_string();
+                Err(TextEmbeddingsError::Backend(BackendError::Inference(
+                    message,
+                )))
+            }
+        }
+        .map_err(|err| {
+            tracing::error!("{err}");
+            ErrorResponse::from(err)
+        })?;
+    
+        // Closure for rerank
+        let rerank_inner = move |query: String, document: String, truncate: bool, infer: Infer| async move {
+            let permit = infer.acquire_permit().await;
+    
+            let response = infer
+                .predict(
+                    (query, document),
+                    truncate,
+                    req.truncation_direction.into(),
+                    req.raw_scores,
+                    permit,
+                )
+                .await
+                .map_err(ErrorResponse::from)?;
+    
+            let score = response.results[0];
+    
+            Ok::<(usize, Duration, Duration, Duration, f32), ErrorResponse>((
+                response.metadata.prompt_tokens,
+                response.metadata.tokenization,
+                response.metadata.queue,
+                response.metadata.inference,
+                score,
+            ))
+        };
+    
+        let truncate = req.truncate.unwrap_or(info.auto_truncate);
+    
+        let (response, metadata) = {
+            let counter = metrics::counter!("te_request_count", "method" => "batch");
+            counter.increment(1);
+    
+            let batch_size = req.documents.len();
+            if batch_size > info.max_client_batch_size {
+                let message = format!(
+                    "batch size {batch_size} > maximum allowed batch size {}",
+                    info.max_client_batch_size
+                );
+                tracing::error!("{message}");
+                let err = ErrorResponse {
+                    error: message,
+                    error_type: ErrorType::Validation,
+                };
+                let counter = metrics::counter!("te_request_failure", "err" => "batch_size");
+                counter.increment(1);
+                Err(err)?;
+            }
+    
+            let mut futures = Vec::with_capacity(batch_size);
+            let query_chars = req.query.chars().count();
+            let mut compute_chars = query_chars * batch_size;
+    
+            for document in &req.documents {
+                compute_chars += document.chars().count();
+                let local_infer = infer.clone();
+                futures.push(rerank_inner(
+                    req.query.clone(),
+                    document.clone(),
+                    truncate,
+                    local_infer.0,
+                ))
+            }
+            let res = join_all(futures)
+                .await
+                .into_iter()
+                .collect::<Result<Vec<(usize, Duration, Duration, Duration, f32)>, ErrorResponse>>()?;
+    
+            let mut results = Vec::with_capacity(batch_size);
+            let mut total_tokenization_time = 0;
+            let mut total_queue_time = 0;
+            let mut total_inference_time = 0;
+            let mut total_compute_tokens = 0;
+    
+            for (index, r) in res.into_iter().enumerate() {
+                total_compute_tokens += r.0;
+                total_tokenization_time += r.1.as_nanos() as u64;
+                total_queue_time += r.2.as_nanos() as u64;
+                total_inference_time += r.3.as_nanos() as u64;
+                let document = if req.return_documents {
+                    Some(DocumentWrapper {
+                        text: req.documents[index].clone(),
+                    })
+                } else {
+                    None
+                };
+
+                let relevance_score = r.4;
+                // Check that s is not NaN or the partial_cmp below will panic
+                if relevance_score.is_nan() {
+                    Err(ErrorResponse {
+                        error: "score is NaN".to_string(),
+                        error_type: ErrorType::Backend,
+                    })?;
+                }
+    
+                results.push(OpenAIRank { index, document, relevance_score })
+            }
+    
+            // Reverse sort
+            results.sort_by(|x, y| x.relevance_score.partial_cmp(&y.relevance_score).unwrap());
+            results.reverse();
+
+            if let Some(top_n) = req.top_n {
+                results.truncate(top_n);
+            }
+    
+            let batch_size = batch_size as u64;
+    
+            let counter = metrics::counter!("te_request_success", "method" => "batch");
+            counter.increment(1);
+    
+            (
+                OpenAICompatRerankResponse{results},
+                ResponseMetadata::new(
+                    compute_chars,
+                    total_compute_tokens,
+                    start_time,
+                    Duration::from_nanos(total_tokenization_time / batch_size),
+                    Duration::from_nanos(total_queue_time / batch_size),
+                    Duration::from_nanos(total_inference_time / batch_size),
+                ),
+            )
+        };
+    
+        metadata.record_span(&span);
+        metadata.record_metrics();
+    
+        let headers = HeaderMap::from(metadata);
+    
+        tracing::info!("Success");
+    
+        Ok((headers, Json(response)))
+    }
+
 /// Tokenize inputs
 #[utoipa::path(
 post,
@@ -1641,7 +1842,10 @@ pub async fn run(
     EmbedSparseResponse,
     RerankRequest,
     Rank,
+    OpenAIRank,
+    DocumentWrapper,
     RerankResponse,
+    OpenAICompatRerankResponse,
     EmbedRequest,
     EmbedResponse,
     ErrorResponse,
@@ -1733,6 +1937,7 @@ pub async fn run(
         .route("/embed_sparse", post(embed_sparse))
         .route("/predict", post(predict))
         .route("/rerank", post(rerank))
+        .route("/v1/rerank", post(openai_rerank))
         .route("/similarity", post(similarity))
         .route("/tokenize", post(tokenize))
         .route("/decode", post(decode))
@@ -1825,8 +2030,7 @@ pub async fn run(
         routes = routes.layer(axum::middleware::from_fn(auth));
     }
 
-    let app = Router::new()
-        .merge(SwaggerUi::new("/docs").url("/api-doc/openapi.json", doc))
+    let mut app = Router::new()
         .merge(routes)
         .merge(public_routes)
         .layer(Extension(infer))
@@ -1839,6 +2043,14 @@ pub async fn run(
         .layer(DefaultBodyLimit::max(payload_limit))
         .layer(cors_layer);
 
+    if let Ok(swagger_ui) = std::env::var("ENABLE_SWAGGER_UI") {
+        tracing::info!("try to set swagger ui");
+        let _swagger_ui_on = String::from("true");
+            match swagger_ui.to_lowercase() {
+            _swagger_ui_on =>
+                app =app.merge(SwaggerUi::new("/docs").url("/api-doc/openapi.json", doc))
+            }
+        }
     // Run server
     let listener = tokio::net::TcpListener::bind(&addr)
         .await
@@ -1899,3 +2111,4 @@ impl From<serde_json::Error> for ErrorResponse {
         }
     }
 }
+
diff --git a/router/src/http/types.rs b/router/src/http/types.rs
index 6012288..85aca6b 100644
--- a/router/src/http/types.rs
+++ b/router/src/http/types.rs
@@ -271,6 +271,26 @@ pub(crate) struct Rank {
 #[derive(Serialize, ToSchema)]
 pub(crate) struct RerankResponse(pub Vec<Rank>);
 
+#[derive(Serialize, ToSchema)]
+pub(crate) struct DocumentWrapper {
+    #[schema(example = "Deep Learning is ...")]
+    pub text: String,
+}
+
+#[derive(Serialize, ToSchema)]
+pub(crate) struct OpenAIRank {
+    #[schema(example = "0")]
+    pub index: usize,
+    #[schema(nullable = true, example = r#"{"text": "Deep Learning is ..."}"#, default = "null")]
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub document: Option<DocumentWrapper>,
+    #[schema(example = "1.0")]
+    pub relevance_score: f32,
+}
+
+#[derive(Serialize, ToSchema)]
+pub(crate) struct OpenAICompatRerankResponse{pub results: Vec<OpenAIRank>}
+
 #[derive(Deserialize, ToSchema, Debug)]
 #[serde(untagged)]
 pub(crate) enum InputType {
@@ -325,6 +345,29 @@ pub(crate) struct OpenAICompatRequest {
     pub encoding_format: EncodingFormat,
 }
 
+#[derive(Deserialize, ToSchema)]
+pub(crate) struct OpenAICompatRerankRequest {
+    #[schema(example = "What is Deep Learning?")]
+    pub query: String,
+    #[schema(example = json!(["Deep Learning is ..."]))]
+    pub documents: Vec<String>,
+    #[serde(default)]
+    #[schema(default = "false", example = "false", nullable = true)]
+    pub truncate: Option<bool>,
+    #[serde(default)]
+    #[schema(default = "right", example = "right")]
+    pub truncation_direction: TruncationDirection,
+    #[serde(default)]
+    #[schema(default = "false", example = "false")]
+    pub raw_scores: bool,
+    #[serde(default)]
+    #[schema(default = "false", example = "false")]
+    pub return_documents: bool,
+    #[serde(default)]
+    #[schema(example = 3, minimum = 1, nullable = true)]
+    pub top_n: Option<usize>,
+}
+
 #[derive(Serialize, ToSchema)]
 #[serde(untagged)]
 pub(crate) enum Embedding {
@@ -590,3 +633,4 @@ pub(crate) enum VertexPrediction {
 pub(crate) struct VertexResponse {
     pub predictions: Vec<VertexPrediction>,
 }
+
diff --git a/router/src/lib.rs b/router/src/lib.rs
index f1b8ba2..0cdb23c 100644
--- a/router/src/lib.rs
+++ b/router/src/lib.rs
@@ -36,6 +36,7 @@ use tokenizers::processors::sequence::Sequence;
 use tokenizers::processors::template::TemplateProcessing;
 use tokenizers::{PostProcessorWrapper, Tokenizer};
 use tracing::Span;
+use std::env;
 
 pub use logging::init_logging;
 
@@ -109,24 +110,42 @@ pub async fn run(
     let backend_model_type = get_backend_model_type(&config, &model_root, pooling)?;
 
     // Info model type
+    let is_rerank = std::env::var("IS_RERANK").unwrap_or_default() == "1";
     let model_type = match &backend_model_type {
+        text_embeddings_backend::ModelType::Classifier if is_rerank => {
+            // 如果环境变量标志为 RERANK，直接走 reranker 分支，不依赖 config.json 字段
+            let mut id2label = std::collections::HashMap::new();
+            id2label.insert("0".to_string(), "LABEL_0".to_string());
+
+            let mut label2id = std::collections::HashMap::new();
+            label2id.insert("LABEL_0".to_string(), 0);
+
+            let classifier_model = ClassifierModel { id2label, label2id };
+            ModelType::Reranker(classifier_model)
+        }
+
         text_embeddings_backend::ModelType::Classifier => {
+            // 原始 classifier 分支，检查字段是否存在
             let id2label = config
                 .id2label
                 .context("`config.json` does not contain `id2label`")?;
+            let label2id = config
+                .label2id
+                .context("`config.json` does not contain `label2id`")?;
+
             let n_classes = id2label.len();
             let classifier_model = ClassifierModel {
                 id2label,
-                label2id: config
-                    .label2id
-                    .context("`config.json` does not contain `label2id`")?,
+                label2id,
             };
+
             if n_classes > 1 {
                 ModelType::Classifier(classifier_model)
             } else {
                 ModelType::Reranker(classifier_model)
             }
         }
+
         text_embeddings_backend::ModelType::Embedding(pool) => {
             ModelType::Embedding(EmbeddingModel {
                 pooling: pool.to_string(),
@@ -249,11 +268,14 @@ pub async fn run(
         .await
         .context("Model backend is not healthy")?;
 
-    tracing::info!("Warming up model");
-    backend
-        .warmup(max_input_length, max_batch_tokens, max_batch_requests)
-        .await
-        .context("Model backend is not healthy")?;
+    if !backend.padded_model {
+        tracing::info!("Warming up model");
+        let max_batch_requests = Some(3);
+        backend
+            .warmup(4, 4, max_batch_requests)
+            .await
+            .context("Model backend is not healthy")?;
+    }
 
     let max_batch_requests = backend
         .max_batch_size
@@ -362,11 +384,11 @@ fn get_backend_model_type(
             continue;
         }
 
-        if Some(text_embeddings_backend::Pool::Splade) == pooling && arch.ends_with("MaskedLM") {
+        if Some(text_embeddings_backend::Pool::Splade) == pooling && (arch.ends_with("MaskedLM") || arch.ends_with("RobertaModel")) {
             return Ok(text_embeddings_backend::ModelType::Embedding(
                 text_embeddings_backend::Pool::Splade,
             ));
-        } else if arch.ends_with("Classification") {
+        } else if arch.ends_with("Classification") || env::var("IS_RERANK").is_ok() {
             if pooling.is_some() {
                 tracing::warn!(
                     "`--pooling` arg is set but model is a classifier. Ignoring `--pooling` arg."
diff --git a/router/src/main.rs b/router/src/main.rs
index 39b975d..2966cd4 100644
--- a/router/src/main.rs
+++ b/router/src/main.rs
@@ -48,7 +48,7 @@ struct Args {
     /// The maximum amount of concurrent requests for this particular deployment.
     /// Having a low limit will refuse clients requests instead of having them
     /// wait for too long and is usually good to handle backpressure correctly.
-    #[clap(default_value = "512", long, env)]
+    #[clap(default_value = "64", long, env)]
     max_concurrent_requests: usize,
 
     /// **IMPORTANT** This is one critical control to allow maximum usage
